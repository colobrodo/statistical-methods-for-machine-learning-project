\newpage
\section{Support Vector Machine}
% short description of the algorithm
The Support Vector Machine (SVM) algorithm learn linear classifiers, finding a linear classifier that is the \textbf{maximum margin separator hyperplane} and so achive the maximum margin from all the point in the training set.\\

Given a linearly separable training set $S = \{(\boldsymbol{x}_1, y_1), \dots, (\boldsymbol{x}_n, y_n)\} \in \mathbb{R}^d \times \{-1, 1\}$ it's possible to find this hyperplane solving the following convex optimization problem with linear constraints.\\ 
\begin{align*} 
    & \underset{\boldsymbol{w} \in \mathbb{R}^d}{\min} \ \frac{1}{2} \Vert \boldsymbol{w} \Vert^2 \\
    & \text{s.t.} \ y_t \boldsymbol{w}^\top \boldsymbol{x}_t \geq 1 \ \text{for} \ t = 1, \dots, m 
\end{align*}\\


In case the training data is not separable, we try to minimise both how much of each constraint is violated and the margin of the separator.\\
We express this as another convex problem using the slack variables $\xi_t$ and a regularisation coefficient $\lambda$.
If the regularisation coefficient is large, the algorithms will generate a predictor that allows more classification error in the training set.\\
Conversely, if $\lambda$ is small, we try to minimise the classification error we have made.\\
Usually for $\lambda$ too small, we try to minimise the misclassification, so the training error is small and we are likely to overfit.\\
Instead, for choices of $\lambda$ too large, we have a high training error, and if the test error is also high, we will underfit. \\
In the next subsection \ref{sub:hyptun}, I describe how I choose the hyperparameter of the regularisation coefficient and how the test and training errors vary when I change it more precisely.\\ 

\begin{align*}
    \underset{(\boldsymbol{w}, \boldsymbol{\xi}) \in \mathbb{R}^{d+m}}{\min} \quad & \frac{\lambda}{2} \Vert \boldsymbol{w} \Vert^2 + \frac{1}{m} \sum_{t = 1}^m \xi_t \\
    \text{s.t.} \quad & y_t \boldsymbol{w}^\top \boldsymbol{x}_t \geq 1 - \xi_t & t = 1, \dots, m \\
    & \xi_t \geq 0 \ \text{for} & t = 1, \dots, m
\end{align*}

Now, fix $\boldsymbol{w} \in \mathbb{R}^d$, we can see $\xi_t = \left[1 - y_t \boldsymbol{w}^\top \boldsymbol{x}_t \right]_+$ which is the hinge loss  $h_{t}(\boldsymbol{w})$.\\

The SVM problem can be rewritten as $$\underset{\boldsymbol{w} \in \mathbb{R}^d}{\min} \ \frac{\lambda}{2} \Vert \boldsymbol{w} \Vert^2 + \frac{1}{m} \sum_{t = 1}^m h_{t}(\boldsymbol{w})$$.\\

The optimization problem that describe the Support Vector Machine is optimized using the Pegasos algorithm.\\
The Pegasos algorithm is a variant of the Stocastic Gradient Descent algorithm, where at each step a point (or a set of points in the mini-batch variant) is sampled randomly from the training set
and the current predictor is updated with the negative gradient of the loss of that training example weighted by a learning rate factor $\eta_t$.\\
In case of Pegasos the learning rate factor $\eta_t$ is choose at each step as $\frac{1}{\lambda t}$.\\
I implement the Pegasos algorithm using the standard variant with the \textit{hinge} loss, and with the \textit{logistic} loss (Described in the Logistic regression subsection \ref{sub:logreg}).\\
Both this functions are convex upper bounds of the zero-one loss, and the $\lambda$ regularization parameter allow to have a $\lambda$-strongly convex function to minimize with the gradient descent.\\
% TODO: refer to the OGD and Pegasos bound on function minimum this is why we say that the function to minimize are $\lambda$-strongly convex

\subsection{Naive}
As I previously said I implemented Pegasos using two surrogate losses: hinge loss and logistic loss.\\
Now I describe the implementation with hinge loss, that differs from the logistic one only by the update step.\\ 
Recall that the hinge loss is defined as $l(y, \hat{y}) = \max\{0, 1 - y_t \hat{y}\}$\\
Given $Z_t = (X_t, Y_t)$ a random sample from the training set, the update rule for Pegasos is:\\
$$\boldsymbol{w}_{t+1} = \boldsymbol{w}_t - \eta_t \nabla\ell_{Z_t}(w_t)$$

Let be $s_t$ the realization for the random variable $Z_t$\\ 
Where $\ell_{s_t}(w) = \left[1 - y_{s_t} \boldsymbol{w}^T x_{s_t}\right]_+ + \frac{\lambda}{2} ||w||^2$ so
$$\nabla\ell_{s_t}(w) = -y_{s_t} x_{s_t} \mathbb{I}\{h_{s_t}(\boldsymbol{w}) > 0\} + \lambda w $$
Let $\boldsymbol{v_t} = y_t x_t I\{h_t(\boldsymbol{w_t}) > 0\}$ and choosing $\eta_t = \frac{1}{\lambda t}$ we have
$$\boldsymbol{w}_{t+1} = \boldsymbol{w}_t (1 - \frac{1}{t}) + \frac{1}{\lambda t} \boldsymbol{v_t}$$

\subsubsection{Implementation details}
I adapted this version of the algorithm from \textit{Pegasos: Primal Estimated sub-GrAdient SOlver for SVM}\cite{Pegasos_paper}\\

\begin{algorithm}[H]
    \SetAlgoLined
    \DontPrintSemicolon
    \caption{Pegasos Algorithm}
    \KwIn{$S$, $\lambda$, $T$}
    \For{$t = 1, 2, \dots, T$}{
        Choose $i_t$ uniformly at random.\\
        Set $\eta_t = \frac{1}{\lambda t}$\\
        \If{$y_{it} \boldsymbol{w}_t^T x_{i_{t}} < 1$}{
            Set $\boldsymbol{w}_{t+1} \leftarrow (1 - \eta_t \lambda)\boldsymbol{w}_t + \eta_t y_{it} \boldsymbol{x}_{i_{t}}$
            }
            \Else {
                Set $\boldsymbol{w}_{t+1} \leftarrow (1 - \eta_t \lambda)\boldsymbol{w}_t$
        }
    }    
    Output $\boldsymbol{w}_{T+1}$
\end{algorithm}

The name of the variables are adapted to be consistent with the pseudo code reported.\\
Between the code presented in the lecture and this one presented in the paper there are some differences:\\
\begin{itemize}
    \item The gradient descent update is written in a slightly different way using a conditional statement instead of the classical indicator function, I choose to remain consistent also with this stilistic choice.\\
    \item Instead of return the average of all the weight vector calculated at each step, the paper returns only the last one.\\ 
    The authors indicate that they note an improvement in performance by returning the last vector instead of the average.\\ 
    I accept also this variation.\\
    \item The Pegasos pseudocode also describes an optional projection step to clamp the magnitude of the linear predictor, but I don't incorporate it.\\ 
\end{itemize}

% TODO: mini-batch: if I test also the mini-batch I should report if I notice some improvment and how the algorithm change based on the hyperparameter
Two other approaches that the author suggest but I don't experiment are:
\begin{itemize}
    \item The author also provides a \textbf{mini-batch} version of the Pegasos algorithm.\\ 
    The chosen batch is a set of $k$ random samples from the training set, and the linear predictor is updated with the average loss gradient of all incorrectly predicted samples in the batch.\\
    The batch size $k$ can be chosen in advance or treated as another hyperparameter.\\
    \item \textbf{sampling without replacement}: a random permutation of the training set is chosen and the updates are performed in order on the new sequence of data.
    In this way, a training point is sampled only once in an epoch.\
    After each epoch, we can choose whether to resume sampling the data sequentially according to the same permutation, or to create a new permutation and sample according to that new order.\\
    Although the authors report that this approach gives better results than uniform sampling as I did, I haven't experimented with this variant of the algorithm.\\    
\end{itemize} 

\subsubsection{Hyperparameter tuning} \label{sub:hyptun}
Unlike the perceptron, the Pegasos algorithm had a hyperparameter to choose from: The regularisation coefficient $\lambda$.\\
To choose the best hyperparameter for this algorithm (and the others that I will implement), I choose the grid search method.\
I divide the training set into 2 different subsets: $S_{traing}$ and $S_{dev}$.\\
The validation set ($S_{dev}$) is used as a surrogate test set to get an estimate of the risk.\\
I choose a finite subset of the possible hyperparameter values ($\Theta_0 \subseteq \Theta$) and for each of them I create a predictor $h_\theta$ which is trained with the chosen hyperparameter on $S_{train}$.\\
The risk is then estimated using the validation error on each predictor, and the one with the lower risk is chosen.\\
The implementation of the grid search in my codebase is in the function 'grid\_search', I used python know arguments to pass a dictionary where the key is the name of the hyperparameter (the same as used as an argument in the training algorithm) and the value is an iterator containing the subset of possible values ($\Theta_0$).\\ 
In this case, I choose the set $\Theta_0 = \{ 0.0001, 0.001, 0.01, 0.1, 1, 10, 100 \}$.\\

\begin{center}
    \begin{tabular}{| c | c | c |}
        \hline
        $\lambda$ & $S_{val}$ & $S_{train}$ \\
        \hline
        0.0001 & 0.3725 & 0.3521666 \\
        \hline
        0.001 & 0.2925 & 0.281333 \\
        \hline
        0.01 & 0.273 & 0.27 \\
        \hline
        \textbf{0.1} & \textbf{0.267} & \textbf{0.2661666} \\
        \hline
        1 & 0.273 & 0.269333 \\
        \hline
        10 & 0.279 & 0.2775 \\
        \hline
        100 & 0.2805 & 0.276333 \\
        \hline
    \end{tabular}
    \\
    The values $S_{val}$ and $S_{train}$ while $\lambda$ changes.\\
\end{center}

Looking at the table we can deduce for which choice of $\lambda$ we underfit and overfit and in our case the sweet spot is the value for $\lambda = 0.1$.\\
The test error for the best predictor is 0.2935.\\
We can also see that for smaller value of lambda we have an high test error and model is overfitting while for large value are likely to underfit.\\
The weight for the Pegasos algorithm are the following: \\
% TODO: file esterno per i pesi del linear predictor
0.2080856  -0.0066276   0.07067274 -0.26477777  0.24795273 -0.06704699\\
0.28821758  0.65130349  0.16619769 -0.06650096 -0.0724

\subsection{Logistic regression} \label{sub:logreg}
Logistic regression aim to learn the function $\eta(\boldsymbol{x}) = \mathbb{P}(Y = +1\ |\ \textbf{X} = \boldsymbol{x})$.\\
The implementation of logistic regression differs from the standard Pegasos because it uses the logistic loss as surrogate loss and not hinge.\\
The logistic loss is defines ad follows: $$\ell(y, \hat{y}) = \log_2 (1+e^{-y\hat{y}}).$$\\
So the gradient descent update becomes: $$\boldsymbol{w}_{t+1} = \boldsymbol{w}_t + \eta_t \sigma(-y_t \boldsymbol{w}^T \boldsymbol{x}_t) y_t \boldsymbol{x}_t$$\\
I choose the regularization parameter as described in the previous subsection and obtained the following results.\\
\begin{center}
    \begin{tabular}{| c | c | c |}
        \hline
        $\lambda$ & $S_{val}$ & $S_{train}$ \\
        \hline
        0.1 & 0.278 & 0.2718333 \\
        \hline
        \textbf{1} & \textbf{0.2735} & \textbf{0.274333} \\
        \hline
        10 & 0.28325 & 0.27816 \\
        \hline
        100 & 0.2805 & 0.278166 \\
        \hline
    \end{tabular}
    \\
    The values $S_{val}$ and $S_{train}$ while $\lambda$ changes for the logistic loss.\\
\end{center}

I have choosen the hyperparameter $\lambda = 1$ and obtained a test error of: 0.292.\\

\subsection{Feature Expansion}
I have also trained the two previous algorithms over the polynomial feature expansion of second degree of the whole dataset.\\
Like in the case of perceptron, since we are able to express a separator in an high dimension we obtain better results than the naive version (at the cost of an increasing number of features in the dataset).\\  
\subsubsection{Pegasos}
This are the following validation and development errors training the Pegasos algorithm the same set of values choosen for the hyperparameter $\lambda$ as in the naive version.\\

\begin{center}
    \begin{tabular}{| c | c | c |}
        \hline
        $\lambda$ & $S_{val}$ & $S_{train}$ \\
        \hline
        \textbf{0.001} & \textbf{0.045} & \textbf{0.039833} \\
        \hline
        0.01 & 0.06 & 0.050166 \\
        \hline
        0.1 & 0.0955 & 0.08833 \\
        \hline
        1 & 0.1725 & 0.1525 \\
        \hline
        10 & 0.219 & 0.211 \\
        \hline
        100 & 0.256 & 0.246 \\
        \hline
        1000 & 0.2635 & 0.25233 \\
        \hline
    \end{tabular}
\end{center}

The test error obtained training the predictor on the whole training set with the regularization coefficent of $0.001$ is $0.053$ with a training error of $0.044125$.\\

\subsubsection{Logistic Regression}

\begin{center}
    \begin{tabular}{| c | c | c |}
        \hline
        $\lambda$ & $S_{val}$ & $S_{train}$ \\
        \hline
        \textbf{0.1} & \textbf{0.11} & \textbf{0.1035} \\
        \hline
        1 & 0.1675 & 0.15 \\
        \hline
        10 & 0.197 & 0.188 \\
        \hline
        100 & 0.2595 & 0.2485 \\
        \hline
        1000 & 0.2505 & 0.2385 \\
        \hline
    \end{tabular}
\end{center}

The test error obtained for the feature expanded logistic regression (with $\lambda = 0.001$) is $0.1125$ with a training error of $0.1055$.\\
Note that for both versions of logistic regression, not all of the $\lambda$ parameters used for the hinge loss SVM in the grid search are used in this case.\\ 
This is because for values that are too small, I get an overflow error when exponentiating in the sigmoid function.\\ 
These values, for which the gradient update calculation is not feasible, are simply discarded.\\
