\newpage
\section{Kernel}
In the previous section I have showed the benefit of training the algorithm in an high-dimensional space.\\
This came with the cost of increasing sensibily the number of the feature even in case of a simple polynomial feature expansion of degree 2.\\
We can have the performance improvement obtained with feature expansion without increasing the dimensionality of the dataset using the kernel trick.\\
Instead of computing the feature expansion map on evry point and then computing the dot product with the expanded linear predictor we can use a kernel $K$ that calculate the dot product of two expanded vector without explicitly calculate the expansion.\\
Let the feature expansion be $\phi(x)$ a kernel $K$ for the feature expansion is defined as $$K(x, x') = \phi(\boldsymbol{x})^T \phi(\boldsymbol{x'})  for all \boldsymbol{x}, \boldsymbol{x'} \in R$$\\
The two kind of kernel used in the project are the \textbf{Polynomial kernel} and the \textbf{Gaussian kernel}.\\
The polynomial kernel of degree $n$ is $$K_n(x, x') = (1 + \boldsymbol{x}^T \boldsymbol{x'})^n$$
The gaussian kernel of parameter $\gamma$ is $$K_\gamma(x, x') = exp(-\frac{1}{2\gamma}||\boldsymbol{x} - \boldsymbol{x'}||^2)$$
Note that both the kernels accept a parameter (a degree in the polynomial case and gamma for gaussian one) this becomes another hyperparameter of the learning algorithm
and it's choosen with the same hyperparameter tuning procedure descripted in the previous chapters.\\ 
Each kernel induce a linear space defined as a set of linear combination of functions $K(\boldsymbol{x}, \dot)$.\\
\begin{align*}
    \mathcal{H}_K \equiv \left\{ \sum_{i=1}^N \alpha_i K(\boldsymbol{x}_i, \cdot) : \boldsymbol{x}_1, \dots, \boldsymbol{x}_N \in \mathcal{X}, \alpha_1, \dots, \alpha_N \in \mathbb{R}, N \in \mathbb{N} \right\}
\end{align*}
This spaces are called Reproducible Kernel Hilbert Space (RHKS).\\
A linear predictor trained in these linear spaces induced by kernels can overfit; in the case of polynomial kernels, we can choose a degree that is too high and obtain a curve with more degrees of freedom, which is more likely to separate the training data and obtain a small training error.\\
For the Gaussian, we should know that the parameter $\gamma$ is the width of the Gaussian centre at the training point.\\
This means that if $\gamma$ is too small, we are likely to have a small training error (we only look at the labels of the nearest points) and overfit.\\


\subsection{Implementation details}
I used two classes with the '\_\_call\_\_' python magic method to implement the kernels functions.\\
Both takes two parameter $X$ and $X2$.\\
They can take two vectors and the kernel behave as mathematicly defined or the parameter can be 2 matrices of size $m$ x $d$ and $n$ x $d$.\\
In the second case a new matrix $\boldsymbol{K}$ ($m$ x $n$) defined as $\boldsymbol{K}_{i, j} = K(X_i, X2_j)$.\\
This is particular useful to avoid python for loops and delegate the computations to the optimize implementation of the numpy primitives.\\

% add the two implementations with code

\subsection{Kernelized Perceptron}

The first algorithm I implement using the kernel trick is the perceptron.\\
following I provide a pseudocode taken from the lecture notes about this algorihtm.\\ 

% TODO: pseudocode
% \begin{algorithm}[H]
%     \SetAlgoLined
%     \DontPrintSemicolon
%     \caption{Pegasos Algorithm}
%     \KwIn{$S$, $\lambda$, $T$}
%     \For{$t = 1, 2, \dots, T$}{
%         Choose $i_t$ uniformly at random.\\
%         Set $\eta_t = \frac{1}{\lambda t}$\\
%         \If{$y_{it} \boldsymbol{w}_t^T x_{i_{t}} < 1$}{
%             Set $\boldsymbol{w}_{t+1} \leftarrow (1 - \eta_t \lambda)\boldsymbol{w}_t + \eta_t y_{it} \boldsymbol{x}_{i_{t}}$
%             }
%             \Else {
%                 Set $\boldsymbol{w}_{t+1} \leftarrow (1 - \eta_t \lambda)\boldsymbol{w}_t$
%         }
%     }    
%     Output $\boldsymbol{w}_{T+1}$
% \end{algorithm}

% Difference between implementation and pseudocode
With the kernel version we have a new hyperparameter: The type of kernel used with its parameter (the degree for polynomials and the $\gamma$ for Gaussians).\\
We choose the best hyperparameter, as described in the previous chapters.\\
Among the possible values, we choose 1, 2, 3, 4 for the possible degrees of the polynomial kernel, and 0.01, 0.1, 1 and 10 for the possible values of $\gamma$.\\

% results
\begin{table}[]
    \begin{tabular}{|c|c|c|c|}
        \hline
        Polynomial Kernel n & Gaussian Kernel $\gamma$ & $S_{val}$ & $S_{train}$ \\ \hline
        1 & & 0.339 & 0.3251666 \\ \hline
    2 & & 0.0685 & 0.06633 \\ \hline
    3 & & 0.0585 & 0.048 \\ \hline
    4 & & 0.06 & 0.031666 \\ \hline
    & 0.01 & 0.2075 & 0.0 \\ \hline
    & 0.1 & 0.1805 & 0.0 \\ \hline
    & 1  & 0.0685 & 0.0015 \\ \hline
     & 10 & 0.072 & 0.01366 \\ \hline    
    \end{tabular}
    \label{tab:kper}
\end{table}
We can make some considerations about the data in the table \ref{tab:kper} explained in the previous theoretical section of this chapter:
% TODO: better rewrite
We can see that when using the Gaussian kernel, we have a training error $S_{train} = 0$ for the smaller value of $\gamma$, 
This can be explained by observing that if the radius of the Gaussians is too small, the Gaussians over the training points do not interact and for each training point the algorithm behaves as a K-NN with $K = 1$ and for the same reason the training error is 0.\\
It's also possible to know in which cases for the polynomial kernel we overfit, observe that in the case of $n = 4$ we have a value $S_{val}$ greater than $S_{train}$.\\
In contrast, for $n < 3$ we also have a high $S_{val}$, indicating underfitting.\\
\subsection{Kernelized Pegasos}

\begin{algorithm}[H]
    \SetAlgoLined
    \DontPrintSemicolon
    \caption{Pegasos Algorithm}
    I implement the kernelized version of the Pegasos algorithm following the pseudocode from the paper, reported here:
    \KwIn{$S$, $\lambda$, $T$}
    Initialize: Set $\boldsymbol{\alpha}_1 = 0$\\
    \For{$t = 1, 2, \dots, T$}{
        Choose $i_t \in {0, \dots, |S|}$ uniformly at random.\\
        
        For all $j \neq i_t$, set $\boldsymbol{\alpha}_{t+1}[j] = \boldsymbol{\alpha}_t[j] $\\
        \If{$y_{i_{t}} \frac{1}{\lambda t} \sum_{j}{\boldsymbol{\alpha}_t[j] y_{i_t} K(\boldsymbol{x}_{i_t}, x_j)} < 1$}{
            Set $\boldsymbol{\alpha}_{t+1}[i_t] = \boldsymbol{\alpha}_t[i_t] + 1$
        } 
        \Else {
            Set $\boldsymbol{\alpha}_{t+1}[i_t] = \boldsymbol{\alpha}_t[i_t]$
        }

    }
    Output $\boldsymbol{\alpha}_{T+1}$\\
\end{algorithm}

% results
As in the previous algorithm, I fixed a number of rounds $T = 100000$ and tuned the algorithm over the hyperparameters $\lambda$ (the regularization coefficient) and a kernel that can be either polynomial of degree $1,2,3$ or $4$, or gaussian with a $\gamma$ parameter of one between ${0.01, 0.1, 1, 10}$.\\
\begin{table}[]
    \begin{tabular}{|l|l|l|l|l|}
    \hline
    $\lambda$ & Polynomial Kernel n & Gaussian Kernel $\gamma$ & $S_{val}$ & $S_{train}$ \\ \hline
    0.001 & 1 &  & 0.275 & 0.2738333333333333 \\ \hline
    0.001 & 2 &  & 0.0605 & 0.055 \\ \hline
    0.001 & 3 &  & 0.053 & 0.045 \\ \hline
    0.001 & 4 &  & 0.057 & 0.026166666666666668 \\ \hline
    0.001 & & 0.01 & 0.1675 & 0.0 \\ \hline
    0.001 & & 0.1 & 0.1465 & 0.0 \\ \hline
    0.001 & & 1 & 0.114 & 0.06633333333333333 \\ \hline
    0.001 & & 10 & 0.157 & 0.13683333333333333 \\ \hline
    0.01 & 1 &  & 0.2755 & 0.26916666666666667 \\ \hline
    0.01 & 2 &  & 0.055 & 0.04833333333333333 \\ \hline
    0.01 & 3 &  & 0.049 & 0.03716666666666667 \\ \hline
    0.01 & 4 &  & 0.051 & 0.025 \\ \hline
    0.01 & & 0.01 & 0.167 & 0.0 \\ \hline
    0.01 & & 0.1 & 0.15 & 0.0 \\ \hline
    0.01 & & 1 & 0.2275 & 0.21266666666666667 \\ \hline
    0.01 & & 10 & 0.2495 & 0.246 \\ \hline
    0.1 & 1 &  & 0.2645 & 0.26316666666666666 \\ \hline
    0.1 & 2 &  & 0.084 & 0.07616666666666666 \\ \hline
    \textbf{0.1} & \textbf{3} &  & \textbf{0.044} & \textbf{0.043} \\ \hline
    0.1 & 4 &  & 0.05 & 0.023166666666666665 \\ \hline
    0.1 & & 0.01 & 0.166 & 0.0 \\ \hline
    0.1 & & 0.1 & 0.1465 & 0.0 \\ \hline
    0.1 & & 1 & 0.2385 & 0.2255 \\ \hline
    0.1 & & 10 & 0.2545 & 0.257 \\ \hline
    1 & 1 & & 0.2725 & 0.2668333333333333 \\ \hline
    1 & 2 & & 0.144 & 0.131 \\ \hline
    1 & 3 & & 0.0825 & 0.072 \\ \hline
    1 & 4 & & 0.0535 & 0.029 \\ \hline
    1 & & 0.01 & 0.167 & 0.0 \\ \hline
    1 & & 0.1 & 0.1445 & 0.0 \\ \hline
    1 & & 1 & 0.2415 & 0.2245 \\ \hline
    1 & & 10 & 0.26 & 0.25883333333333336 \\ \hline
    10 & 1 & & 0.285 & 0.2796666666666667 \\ \hline
    10 & 2 & & 0.193 & 0.17766666666666667 \\ \hline
    10 & 3 & & 0.143 & 0.13383333333333333 \\ \hline
    10 & 4 & & 0.0705 & 0.05433333333333333 \\ \hline
    10 & & 0.01 & 0.1675 & 0.0 \\ \hline
    10 & & 0.1 & 0.1445 & 0.0 \\ \hline
    10 & & 1 & 0.2365 & 0.225 \\ \hline
    10 & & 10 & 0.274 & 0.265 \\ \hline
    100 & 1 & & 0.2805 & 0.27616666666666667 \\ \hline
    100 & 2 & & 0.2015 & 0.19333333333333333 \\ \hline
    100 & 3 & & 0.2185 & 0.21433333333333332 \\ \hline
    100 & 4 & & 0.111 & 0.10033333333333333 \\ \hline
    100 & & 0.01 & 0.169 & 0.0 \\ \hline
    100 & & 0.1 & 0.144 & 0.0 \\ \hline
    100 & & 1 & 0.2405 & 0.223 \\ \hline
    100 & & 10 & 0.2845 & 0.2813333333333333 \\ \hline
\end{tabular}
\label{tab:kpeg}
\end{table}

% 
The best hyperparameters obtained by the grid search procedure are a regularization coefficient $\lambda = 0.1$ and a polynomial kernel of degree 3.\\
We obtain a training error on the whole data set of 0.035875 and a test error of 0.043.\\
We can see from the test error that this is the best performing method implemented.\\
This is also because we are using a 3rd degree polynomial kernel and we are training a linear predictor in a higher dimensional space than the one I obtained with the 2nd degree polynomial expansion.\\
In the table \ref{tab:kpeg} we can also see for which values of $\lambda$ and kernel degree the predictor overfits, fixing the other hyperparameter.\\
Fixing the regularisation coefficient $\lambda = 0.1$ we can experimentally confirm that for high degrees of the kernel (4) the predictor overfits and has a high validation error and for smaller values (1 or 2) it underfits.\\
Conversely, by fixing the chosen kernel and varying $\lambda$, we can see that $S_{val}$ is slightly higher for values of the regularisation coefficient $< 0.1$ and we are probably overfitting, while for higher values the validation error is much higher and this can signal underfitting.\\
