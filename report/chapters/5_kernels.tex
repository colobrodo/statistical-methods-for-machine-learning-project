\newpage
\section{Kernel}
In the previous section I have showed the benefit of training the algorithm in an high-dimensional space.\\
This came with the cost of increasing sensibily the number of the feature even in case of a simple polynomial feature expansion of degree 2.\\
We can have the performance improvement obtained with feature expansion without increasing the dimensionality of the dataset using the kernel trick.\\
Instead of computing the feature expansion map on evry point and then computing the dot product with the expanded linear predictor we can use a kernel $K$ that calculate the dot product of two expanded vector without explicitly calculate the expansion.\\
Let the feature expansion be $\phi(x)$ a kernel $K$ for the feature expansion is defined as $$K(x, x') = \phi(\boldsymbol{x})^T \phi(\boldsymbol{x'})  for all \boldsymbol{x}, \boldsymbol{x'} \in R$$\\
The two kind of kernel used in the project are the \textbf{Polynomial kernel} and the \textbf{Gaussian kernel}.\\
The polynomial kernel of degree $n$ is $$K_n(x, x') = (1 + \boldsymbol{x}^T \boldsymbol{x'})^n$$
The gaussian kernel of parameter $\gamma$ is $$K_\gamma(x, x') = exp(-\frac{1}{2\gamma}||\boldsymbol{x} - \boldsymbol{x'}||^2)$$
Note that both the kernels accept a parameter (a degree in the polynomial case and gamma for gaussian one) this becomes another hyperparameter of the learning algorithm
and it's choosen with the same hyperparameter tuning procedure descripted in the previous chapters.\\ 
Each kernel induce a linear space defined as a set of linear combination of functions $K(\boldsymbol{x}, \dot)$.\\
\begin{align*}
    \mathcal{H}_K \equiv \left\{ \sum_{i=1}^N \alpha_i K(\boldsymbol{x}_i, \cdot) : \boldsymbol{x}_1, \dots, \boldsymbol{x}_N \in \mathcal{X}, \alpha_1, \dots, \alpha_N \in \mathbb{R}, N \in \mathbb{N} \right\}
\end{align*}
This spaces are called Reproducible Kernel Hilbert Space (RHKS).\\
A linear predictor trained in these linear spaces induced by kernels can overfit; in the case of polynomial kernels, we can choose a degree that is too high and obtain a curve with more degrees of freedom, which is more likely to separate the training data and obtain a small training error.\\
For the Gaussian, we should know that the parameter $\gamma$ is the width of the Gaussian centre at the training point.\\
This means that if $\gamma$ is too small, we are likely to have a small training error (we only look at the labels of the nearest points) and overfit.\\


\subsection{Implementation details}
I used two classes with the '\_\_call\_\_' python magic method to implement the kernels functions.\\
Both takes two parameter $X$ and $X2$.\\
They can take two vectors and the kernel behave as mathematicly defined or the parameter can be 2 matrices of size $m x d$ and $n x d$.\\
In the second case a new matrix $\boldsymbol{K}$ ($m x n$) defined as $\boldsymbol{K}_{i, j} = K(X_i, X2_j)$.\\
This is particular useful to avoid python for loops and delegate the computations to the optimize implementation of the numpy primitives.\\

% add the two implementations with code

\subsection{Kernelized Perceptron}
% pseudocode
% results
\subsection{Kernelized Pegasos}
% pseudocode
% results
