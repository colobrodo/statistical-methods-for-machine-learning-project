\newpage
\section{Kernel}
In the previous section I have showed the benefit of training the algorithms in an high-dimensional space.\\
This came with the cost of increasing sensibily the number of the feature even in case of a simple polynomial feature expansion of degree 2.\\
We can have the performance improvement obtained with feature expansion without increasing the dimensionality of the dataset using the kernel trick.\\
Instead of computing the feature expansion map on evry point and then computing the dot product with the expanded linear predictor we can use a kernel $K$ that calculate the dot product of two expanded vector without explicitly calculate the expansion.\\
Let the feature expansion be $\phi(x)$, a kernel $K$ for the feature expansion is defined as $$K(x, x') = \phi(\boldsymbol{x})^T \phi(\boldsymbol{x'}) \quad \text{for all} \boldsymbol{x}, \boldsymbol{x'} \in R$$\\
The two kind of kernel used in the project are the \textbf{Polynomial kernel} and the \textbf{Gaussian kernel}.\\
The \textit{polynomial kernel} of degree $n$ is defined as $$K_n(x, x') = (1 + \boldsymbol{x}^T \boldsymbol{x'})^n$$
The \textit{gaussian kernel} of parameter $\gamma$ is defined as $$K_\gamma(x, x') = exp(-\frac{1}{2\gamma}||\boldsymbol{x} - \boldsymbol{x'}||^2)$$
Note that both the kernels accept a parameter (a degree in the polynomial case and gamma for gaussian one) this becomes another hyperparameter of the learning algorithm
and it's choosen with the same hyperparameter tuning procedure descripted in the previous chapters.\\ 
Each kernel induce a linear space defined as a set of linear combination of functions $K(\boldsymbol{x}, \cdot)$.\\
\begin{align*}
    \mathcal{H}_K \equiv \left\{ \sum_{i=1}^N \alpha_i K(\boldsymbol{x}_i, \cdot) : \boldsymbol{x}_1, \dots, \boldsymbol{x}_N \in \mathcal{X}, \alpha_1, \dots, \alpha_N \in \mathbb{R}, N \in \mathbb{N} \right\}
\end{align*}
This spaces are called Reproducible Kernel Hilbert Space (RHKS).\\
A linear predictor trained in these linear spaces induced by kernels can overfit; in the case of polynomial kernels, we can choose a degree that is too high and obtain a linear separator that is also rapresentable by a polynomial curve of degree $n$ in the original space, which is more likely to separate the training data and obtain a small training error.\\
For the Gaussian, we should know that the parameter $\gamma$ is the width of a Gaussian centered on the training point.\\
If $\gamma$ is too small, the training error will be small and we will overfit because the Gaussians of different training points almost never overlap and the algorithm behaves like K nearest neighbours with $K =$.\\


\subsection{Implementation details}
I used two classes with the '\_\_call\_\_' python magic method to implement the kernels functions.\\
Both takes two parameter $X$ and $X2$.\\
They can take two vectors and the kernel behave as mathematicly defined or the parameter can be 2 matrices of size $m$ x $d$ and $n$ x $d$.\\
In the second case a returned matrix $\boldsymbol{K}$ ($m$ x $n$) is defined as $\boldsymbol{K}_{i, j} = K(X_i, X2_j)$.\\
This is particularly useful when calculating the kernel over the whole dataset, to avoid Python for loops and delegate the calculations to the optimised implementation of the numpy primitives.\\

Following the implementation of the call method for the two Kernel classes.\\
Implementation of the polynomial kernel over $X$ and $X2$.\\
\begin{python}
def __call__(self, X: np.ndarray, X2: np.ndarray):
    if X2.ndim == 1:
        return np.power(np.dot(X, X2) + 1, self.degree)
    elif X2.ndim == 2:
        return np.power(np.dot(X, X2.T) + 1, self.degree)
\end{python}

Implementation of the gaussian kernel over $X$ and $X2$ with the parameter gamma $\gamma$ .\\
\begin{python}
    def __call__(self, X: np.ndarray, X2: np.ndarray):
        if X2.ndim == 1:
            dist = np.linalg.norm(X - X2, 2, axis=1)
        elif X2.ndim == 2:
            dist = np.linalg.norm(X[:, np.newaxis, :] - X2[np.newaxis, :, :], axis=2)
        return np.exp(-dist / self.gamma)
\end{python}

\subsection{Kernelized Perceptron}

The first algorithm I implement using the kernel trick is the perceptron.\\
Following I provide a pseudocode taken from the lecture notes about this algorihtm.\\ 

% pseudocode

\begin{algorithm}[H]
    \SetAlgoLined
    \DontPrintSemicolon
    \caption{Kernel Perceptron}
    $S \leftarrow \emptyset$ \\
    \For{$t = 1, 2, \dots$}{
        Get next example $(\boldsymbol{x}_t, y_t)$ \\
        Compute $\hat{y_t} = \text{sgn}\left(\sum_{s \in S}\ y_s K(\boldsymbol{x}_s, \boldsymbol{x}_t) \right)$ \\
        \If{$\hat{y_t} \neq y_t$}{
            $S \leftarrow S \cup \{t\}$
        }
    }
\end{algorithm}

% Difference between implementation and pseudocode
Looking at the implementation, I have implemented the kernel perceptron algorithm with an array $\alpha$ of integer weights for each point of the training set instead of using a set $S$, this may seem different from the pseudocode presented, but we can interpret $\alpha$ as representing a multi-set and have an equivalent algorithm.\\
This choice, taken from the pseudo code of the kernelized perceptron in the Pegasos paper \cite{Pegasos_paper}, is adopted because we can express the summation more simply and efficiently thanks to the numpy primitive such as 'np.dot'.\\

% Hyperparameter
With the kernel version we have a new hyperparameter: The type of kernel used with its parameter (the degree for polynomials and the $\gamma$ for Gaussians).\\
We choose the best hyperparameter, as described in the previous chapters.\\
Among the possible values, we choose 1, 2, 3, 4 for the possible degrees of the polynomial kernel, and 0.01, 0.1, 1 and 10 for the possible values of $\gamma$.\\

% results
\begin{table}[]
    \begin{tabular}{|c|c|c|c|}
        \hline
        Polynomial Kernel n & Gaussian Kernel $\gamma$ & $S_{val}$ & $S_{train}$ \\ \hline
        1 & & 0.339 & 0.3251666 \\ \hline
    2 & & 0.0685 & 0.06633 \\ \hline
    \textbf{3} & & \textbf{0.0585} & \textbf{0.048} \\ \hline
    4 & & 0.06 & 0.031666 \\ \hline
    & 0.01 & 0.2075 & 0.0 \\ \hline
    & 0.1 & 0.1805 & 0.0 \\ \hline
    & 1  & 0.0685 & 0.0015 \\ \hline
     & 10 & 0.072 & 0.01366 \\ \hline    
    \end{tabular}
    \label{tab:kper}
\end{table}
We can make some considerations about the data in the table \ref{tab:kper} based on the previous theoretical section of this chapter:
As we already explained for smaller values of $\gamma$ in the Gaussian kernel the training error is 0 and the algorithm overfits with this parameters.\\
It's also possible to know in which cases for the polynomial kernel we are overfitting, observe that in the case of $n = 4$ we have a value $S_{val}$ significantly greater than $S_{train}$.\\
In contrast, for $n < 3$ we also have both a high $S_{val}$ and a high $S_{train}$, indicating underfitting.\\
The polynomial kernel with $n = 3$ is the best one even if the $S_{val}$ higher than $S_{train}$ indicates a slight overfitting.\\
After choosing this hyperparameter we obtain  a training error of 0.034125 and a test error of 0.0495.\\

\subsection{Kernelized Pegasos}

\begin{algorithm}[H]
    \SetAlgoLined
    \DontPrintSemicolon
    \caption{Pegasos Algorithm}
    I implement the kernelized version of the Pegasos algorithm following the pseudocode from the paper, reported here:
    \KwIn{$S$, $\lambda$, $T$}
    Initialize: Set $\boldsymbol{\alpha}_1 = 0$\\
    \For{$t = 1, 2, \dots, T$}{
        Choose $i_t \in {0, \dots, |S|}$ uniformly at random.\\
        
        For all $j \neq i_t$, set $\boldsymbol{\alpha}_{t+1}[j] = \boldsymbol{\alpha}_t[j] $\\
        \If{$y_{i_{t}} \frac{1}{\lambda t} \sum_{j}{\boldsymbol{\alpha}_t[j] y_{i_t} K(\boldsymbol{x}_{i_t}, x_j)} < 1$}{
            Set $\boldsymbol{\alpha}_{t+1}[i_t] = \boldsymbol{\alpha}_t[i_t] + 1$
        } 
        \Else {
            Set $\boldsymbol{\alpha}_{t+1}[i_t] = \boldsymbol{\alpha}_t[i_t]$
        }

    }
    Output $\boldsymbol{\alpha}_{T+1}$\\
\end{algorithm}

% results
As in the previous algorithm, I fixed a number of rounds $T = 100000$ and tuned the algorithm over the hyperparameters $\lambda$ (the regularization coefficient) and a kernel that can be either polynomial of degree $1,2,3$ or $4$, or gaussian with a $\gamma$ parameter of one between ${0.01, 0.1, 1, 10}$.\\
\begin{table}[]
    \begin{tabular}{|l|l|l|l|l|}
    \hline
    $\lambda$ & Polynomial Kernel n & Gaussian Kernel $\gamma$ & $S_{val}$ & $S_{train}$ \\ \hline
    0.001 & 1 &  & 0.275 & 0.2738333333333333 \\ \hline
    0.001 & 2 &  & 0.0605 & 0.055 \\ \hline
    0.001 & 3 &  & 0.053 & 0.045 \\ \hline
    0.001 & 4 &  & 0.057 & 0.026166666666666668 \\ \hline
    0.001 & & 0.01 & 0.1675 & 0.0 \\ \hline
    0.001 & & 0.1 & 0.1465 & 0.0 \\ \hline
    0.001 & & 1 & 0.114 & 0.06633333333333333 \\ \hline
    0.001 & & 10 & 0.157 & 0.13683333333333333 \\ \hline
    0.01 & 1 &  & 0.2755 & 0.26916666666666667 \\ \hline
    0.01 & 2 &  & 0.055 & 0.04833333333333333 \\ \hline
    0.01 & 3 &  & 0.049 & 0.03716666666666667 \\ \hline
    0.01 & 4 &  & 0.051 & 0.025 \\ \hline
    0.01 & & 0.01 & 0.167 & 0.0 \\ \hline
    0.01 & & 0.1 & 0.15 & 0.0 \\ \hline
    0.01 & & 1 & 0.2275 & 0.21266666666666667 \\ \hline
    0.01 & & 10 & 0.2495 & 0.246 \\ \hline
    0.1 & 1 &  & 0.2645 & 0.26316666666666666 \\ \hline
    0.1 & 2 &  & 0.084 & 0.07616666666666666 \\ \hline
    \textbf{0.1} & \textbf{3} &  & \textbf{0.044} & \textbf{0.043} \\ \hline
    0.1 & 4 &  & 0.05 & 0.023166666666666665 \\ \hline
    0.1 & & 0.01 & 0.166 & 0.0 \\ \hline
    0.1 & & 0.1 & 0.1465 & 0.0 \\ \hline
    0.1 & & 1 & 0.2385 & 0.2255 \\ \hline
    0.1 & & 10 & 0.2545 & 0.257 \\ \hline
    1 & 1 & & 0.2725 & 0.2668333333333333 \\ \hline
    1 & 2 & & 0.144 & 0.131 \\ \hline
    1 & 3 & & 0.0825 & 0.072 \\ \hline
    1 & 4 & & 0.0535 & 0.029 \\ \hline
    1 & & 0.01 & 0.167 & 0.0 \\ \hline
    1 & & 0.1 & 0.1445 & 0.0 \\ \hline
    1 & & 1 & 0.2415 & 0.2245 \\ \hline
    1 & & 10 & 0.26 & 0.25883333333333336 \\ \hline
    10 & 1 & & 0.285 & 0.2796666666666667 \\ \hline
    10 & 2 & & 0.193 & 0.17766666666666667 \\ \hline
    10 & 3 & & 0.143 & 0.13383333333333333 \\ \hline
    10 & 4 & & 0.0705 & 0.05433333333333333 \\ \hline
    10 & & 0.01 & 0.1675 & 0.0 \\ \hline
    10 & & 0.1 & 0.1445 & 0.0 \\ \hline
    10 & & 1 & 0.2365 & 0.225 \\ \hline
    10 & & 10 & 0.274 & 0.265 \\ \hline
    100 & 1 & & 0.2805 & 0.27616666666666667 \\ \hline
    100 & 2 & & 0.2015 & 0.19333333333333333 \\ \hline
    100 & 3 & & 0.2185 & 0.21433333333333332 \\ \hline
    100 & 4 & & 0.111 & 0.10033333333333333 \\ \hline
    100 & & 0.01 & 0.169 & 0.0 \\ \hline
    100 & & 0.1 & 0.144 & 0.0 \\ \hline
    100 & & 1 & 0.2405 & 0.223 \\ \hline
    100 & & 10 & 0.2845 & 0.2813333333333333 \\ \hline
\end{tabular}
\label{tab:kpeg}
\end{table}

The best hyperparameters obtained by the grid search procedure are a regularization coefficient $\lambda = 0.1$ and a polynomial kernel of degree 3.\\
Note that the best performing kernel is the same both for \textit{kernelized perceptron} and for \textit{Pegasos}.\\ 
We obtain a training error on the whole data set of 0.035875 and a test error of 0.043.\\
We can see from the test error that this is the best performing method implemented.\\
This is also because we are using a 3rd degree polynomial kernel and we are training a linear predictor in a higher dimensional space than the one I obtained with the 2nd degree polynomial expansion.\\
In the table \ref{tab:kpeg} we can also see for which values of $\lambda$ and kernel degree the predictor overfits, fixing the other hyperparameter.\\
Fixing the regularization coefficient $\lambda = 0.1$ we can experimentally confirm that for high degrees of the kernel (4) the predictor overfits and has a high validation error and for smaller values (1 or 2) it underfits.\\
Conversely, by fixing the chosen kernel and varying $\lambda$, we can see that $S_{val}$ is slightly higher for values of the regularization coefficient $< 0.1$ and we are probably overfitting, while for higher values the validation error is much higher and this can signal underfitting.\\

\subsection{A note on the complexity of the algorithms}
The computational cost of the kernelized versions of the algorithms differs significantly from the naive (or feature-extended) ones.\\
In the case of the perceptron, the number of samples considered is the number of epochs times the size of the training set (in non-separable cases like the one considered).\\
While for Pegasos the number of rounds chosen by the user is fixed to $100000$ in our case.\\
The computational complexity for both is linear in the number of samples $T$ considered ($O(T)$).\\
For the kernelized versions, instead, for each sample considered, we should compute a linear combination of the learned $alphas$ and the kernel between the sample and every other point in the training set.\\
This means that the computational complexity increases from $O(T)$ to $O(T * m)$, where $m$ is the size of the training set.\\
The variant of the kernelized algorithm described in the Pegasos pseudocode is more effective for a smaller number of rounds $T \leq m$ (since in the worst case it will be $O(T^2)$), but to get better results I trained the algorithm with a larger value for $T$.\\